{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data used to train a model sits in memory, we can create an input pipeline by constructing a data set using **tf.data.Dataset.from_tensors** or **tf.data.Dataset.from_tensor_slices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, Y, epoch, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between both functions?  \n",
    "**from_tensors**: Combines the input and returns a dataset with a single element  \n",
    "**from_tensor_slices**: Creates a dataset with a separate element for each row of the input tensor  \n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with from_tensors: \n",
      " <TensorDataset shapes: (2, 2), types: tf.int32> \n",
      "\n",
      "Dataset created with from_tensors: \n",
      " <TensorSliceDataset shapes: (2,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "# Tensor that is going to be used to create the datasets\n",
    "t = tf.constant([[4, 2], [5, 3]])\n",
    "\n",
    "ds_tensors = tf.data.Dataset.from_tensors(t)\n",
    "print('Dataset created with from_tensors: \\n', str(ds_tensors), '\\n')\n",
    "\n",
    "ds_tensor_slices = tf.data.Dataset.from_tensor_slices(t)\n",
    "print('Dataset created with from_tensors: \\n', str(ds_tensor_slices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if data is not sitting in memory? Imagine we are trying to read data from a csv file, then we would use **TextLineDataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(records):\n",
    "    cols = tf.decode_csv(records, record_defaults = [[0], ['house'], [0]])\n",
    "    features = {'sq_footage': cols[0], 'type': cols[1]}\n",
    "    label = cols[2]\n",
    "    \n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(csv_file_path):\n",
    "    dataset = tf.data.TextLineDataset(csv_file_path)\n",
    "    dataset = dataset.map(parse_row)\n",
    "    dataset = dataset.shuffle(1000).repeat(15).batch(128)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we are using the past functions with the following data from a csv file.  \n",
    "\n",
    "|sq_footage | property_type | price |\n",
    "| --- | --- | --- |\n",
    "|1001 | house | 501 |\n",
    "|2001 | house | 1001 |\n",
    "|3001 | house | 1501 |\n",
    "|1001 | apt | 701 |\n",
    "|2001 | apt | 1301 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can read a set of shared csv files using **TextLineDataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path):\n",
    "    dataset = tf.data.Dataset.list_files(path)                   \\\n",
    "                             .flat_map(tf.data.TextLineDataset)  \\\n",
    "                             .map(parse_row)\n",
    "    \n",
    "    # List files:\n",
    "    # Getting all file names from the path\n",
    "    \n",
    "    # Flat map:\n",
    "    # Join all csv in a single dataset (one to many transformation)\n",
    "    \n",
    "    # Map:\n",
    "    # Mapping each row (one to one transformation)\n",
    "    \n",
    "    dataset = dataset.shuffle(1000) \\\n",
    "                     .repeat(15)    \\\n",
    "                     .batch(128)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature columns bridge the gap between colyumns in a csv file to the features used to train a model. MODELS WANT TO TRAIN WITH NUMBERS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "featcols = [\n",
    "    tf.feature_column.numeric_column('sq_footage'),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('type',\n",
    "                                                              ['house', 'apt']) # Possible values in []\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the feature column API to determine the features.  \n",
    "**numeric_column** for numerical columns  \n",
    "**categorical_column_with_vocabulary_list** for the property type. Use this when your inputs are in a string or integer format and you have an in memory vocabulary mapping to each value to an integer ID. By default, out of vocabulary values are ignored.  \n",
    "  \n",
    "Under the hood: feature columns take care of packing the inputs intto the input vector of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
